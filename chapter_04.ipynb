{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip install -Uqq fastbook \n",
    "import fastbook \n",
    "fastbook.setup_book()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from fastai.vision.all import * \n",
    "from fastbook import * "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Further Research](https://github.com/fastai/fastbook/blob/master/04_mnist_basics.ipynb)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. Create your own implementation of `Learner` from scratch, based on the "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from fastai.vision.all import *\n",
    "from fastbook import * \n",
    "\n",
    "\n",
    "def mnist_loss(predictions, targets):\n",
    "    predictions = predictions.sigmoid()\n",
    "    return torch.where(targets == 1, 1 - predictions, predictions).mean()\n",
    "\n",
    "\n",
    "def build_dset(path_three, path_seven):\n",
    "    threes = _build_tensor_from_image(path_three)\n",
    "    sevens = _build_tensor_from_image(path_seven)\n",
    "    return _build_dset(threes, sevens)\n",
    "\n",
    "\n",
    "def _build_dset(threes, sevens):\n",
    "    x = torch.cat([threes, sevens])\n",
    "    x = x.view(-1, 28 * 28)\n",
    "    y = tensor([1] * len(threes) + [0] * len(sevens))\n",
    "    y = y.unsqueeze(1)\n",
    "    return list(zip(x, y))\n",
    "    \n",
    "    \n",
    "def _build_tensor_from_image(path):\n",
    "    x = path.ls().sorted()\n",
    "    x = [tensor(Image.open(image_path)) for image_path in x]\n",
    "    return torch.stack(x).float() / 255\n",
    "\n",
    "\n",
    "def batch_accuracy(xb, yb):\n",
    "    predictions = xb.sigmoid()\n",
    "    correct = (predictions > 0.5) == yb\n",
    "    return correct.float().mean()\n",
    "\n",
    "    \n",
    "class BasicOptim:\n",
    "    def __init__(self, params, learning_rate):\n",
    "        self.params = list(params)\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def step(self, *args, **kwargs):\n",
    "        for param in self.params:\n",
    "            param.data -= param.grad.data * self.learning_rate\n",
    "            \n",
    "    def zero_grad(self, *args, **kwargs):\n",
    "        for param in self.params:\n",
    "            param.grad = None \n",
    "            \n",
    "            \n",
    "class CustomLearner:\n",
    "    def __init__(self, dls, model, opt, loss_func, metrics=accuracy):\n",
    "        self.train_dl = dls[0]\n",
    "        self.valid_dl = dls[1]\n",
    "        self.model = model \n",
    "        self.opt = opt\n",
    "        self.loss_func = loss_func\n",
    "        self.metrics = metrics\n",
    "        \n",
    "    def fit(self, n_epochs, learning_rate):\n",
    "        self.train_model(n_epochs)\n",
    "        \n",
    "    def train_model(self, n_epochs):\n",
    "        for _ in range(n_epochs):\n",
    "            self.train_epoch()\n",
    "            print(self.validate_epoch(), end=\"\\n\")\n",
    "            \n",
    "    def train_epoch(self):\n",
    "        for xb, yb in self.train_dl:\n",
    "            self.calc_grad(xb, yb)\n",
    "            self.opt.step()\n",
    "            self.opt.zero_grad()\n",
    "            \n",
    "    def calc_grad(self, xb, yb):\n",
    "        predictions = self.model(xb)\n",
    "        loss = self.loss_func(predictions, yb)\n",
    "        loss.backward()\n",
    "            \n",
    "    def validate_epoch(self):\n",
    "        accuracies = [self.metrics(self.model(xb), yb) for xb, yb in self.valid_dl]\n",
    "        return round(torch.stack(accuracies).mean().item(), 4)\n",
    "\n",
    "path = untar_data(URLs.MNIST_SAMPLE)\n",
    "path.ls()\n",
    "\n",
    "train_dset = build_dset(path/\"train\"/\"3\", path/\"train\"/\"7\")\n",
    "valid_dset = build_dset(path/\"valid\"/\"3\", path/\"valid\"/\"7\")\n",
    "\n",
    "train_dl = DataLoader(train_dset, batch_size=256)\n",
    "valid_dl = DataLoader(valid_dset, batch_size=256)\n",
    "\n",
    "dls = DataLoaders(train_dl, valid_dl)\n",
    "\n",
    "linear_model = nn.Linear(28 * 28, 1)\n",
    "\n",
    "opt = BasicOptim(linear_model.parameters(), 1.)\n",
    "\n",
    "custom_learner = CustomLearner(\n",
    "    dls, \n",
    "    linear_model, \n",
    "    opt=opt,\n",
    "    loss_func=mnist_loss,\n",
    "    metrics=batch_accuracy,\n",
    ")\n",
    "\n",
    "custom_learner.fit(5, 0.1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. Complete all the steps in this chapter using the full MNIST datasets (that is, for all digits, not just 3s and 7s)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from fastai.vision.all import *\n",
    "from fastbook import *\n",
    "\n",
    "# first we call `get_image_files_path`\n",
    "# to get a list of paths to each images in a given dataset\n",
    "\n",
    "# we convert each image from this lists to a tensor,\n",
    "# the result is a list of tensors, where each tensor represents an image\n",
    "\n",
    "# then, we stack all of those tensors togethers into a single tensor \n",
    "# this tensor represent the average of a given category,\n",
    "# if the path was pointing to the `4` dataset,\n",
    "# then, this tensor represents the average of all 4s in the dataset \n",
    "\n",
    "# finally, we each element within the tensor are converted to a float\n",
    "# and divided by 255\n",
    "# so that the intensity of grayness lies between 0 and 1\n",
    "# and not between 0 and 255 anymore.\n",
    "\n",
    "# the `view` methods with the -1 and 28 * 28 parameters are used\n",
    "# to change the tensor's shape from `torch.Size([28, 28])` \n",
    "# to a vector whose shape is `torch.Size([28 * 28])`.\n",
    "\n",
    "# build_x helps us build the dependent variable, `x`,\n",
    "# i.e. the input image which we are trying to categorize.\n",
    "def build_x(path):\n",
    "    x = get_image_files(path).sorted()\n",
    "    x = [tensor(Image.open(image_path)) for image_path in x]\n",
    "    x = torch.stack(x)\n",
    "    x = x.float() / 255\n",
    "    return x.view(-1, 28 * 28)\n",
    "\n",
    "\n",
    "# first we call `get_image_files_path`\n",
    "# to get a list of paths to each images in a given dataset\n",
    "\n",
    "# all of those image_paths are located in a given directory\n",
    "# where the directory represents the category of the image\n",
    "# so all 4s are located under the `4` directory.\n",
    "# for all images in validation/testing data set, we get their label (by infering which directory they belong to)\n",
    "# as a result, we have a long list of labels for each images\n",
    "\n",
    "# then we conver this list of labels into a tensor\n",
    "\n",
    "# each label corresponds to a given x. \n",
    "# this is why we needed to sorted the image path in x and y.\n",
    "# we want to build a dataset with an input image, and its target label.\n",
    "\n",
    "# `build_y` helps us build the independent variable,\n",
    "# i.e. `y`\n",
    "def build_y(path):\n",
    "    y = get_image_files(path).sorted()\n",
    "    y = [int(image_path.parent.name) for image_path in y]\n",
    "    return tensor(y)\n",
    "\n",
    "# `build_dl` zips the lists x and y\n",
    "# it creates a list of tuples\n",
    "# where the first element of the tuple is x, an input image,\n",
    "# and the second element is y, the label of x.\n",
    "\n",
    "# this dataset is then fed into a `DataLoader` with a batch size of 256\n",
    "# I do not know why it needs to be shuffled ...\n",
    "def build_dl(x, y):\n",
    "    dset = list(zip(x, y))\n",
    "    return DataLoader(dset, batch_size=256, shuffle=True)\n",
    "\n",
    "# download the dataset from the dataset's url\n",
    "# get a path to the dataset\n",
    "path = untar_data(URLs.MNIST)\n",
    "path.ls()\n",
    "\n",
    "# build the x and y, (the input image and their labels)\n",
    "# also create a `DataLoader` from the x and y\n",
    "(train_x, train_y) = (build_x(path/\"training\"), build_y(path/\"training\"))\n",
    "train_dl = build_dl(train_x, train_y)\n",
    "\n",
    "# do the same thing as the previous step,\n",
    "# but this time, with the validation data\n",
    "(valid_x, valid_y) = (build_x(path/\"testing\"), build_y(path/\"testing\"))\n",
    "valid_dl = build_dl(valid_x, valid_y)\n",
    "\n",
    "# create a `DataLoaders` instance with `train_dl` and `valid_dl`\n",
    "dls = DataLoaders(train_dl, valid_dl)\n",
    "\n",
    "# use a simple sequential model \n",
    "simple_net = nn.Sequential(\n",
    "    nn.Linear(28 * 28, 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50, 10)\n",
    ")\n",
    "\n",
    "# configure the learner with the dataloaders;\n",
    "# Stochastic Gradient Descent as a optimization function;\n",
    "# since we have more than two categories to predict\n",
    "# we will need to use `nn.CrossEntropyLoss` as a loss function\n",
    "# instead of the binary encoding that we used previously with `mnist_loss`;\n",
    "# accuracy will be used as our metrics\n",
    "# the learner's default learning_rate will be 0.1\n",
    "learner = Learner(\n",
    "    dls, \n",
    "    simple_net, \n",
    "    opt_func=SGD, \n",
    "    loss_func=nn.CrossEntropyLoss(),\n",
    "    metrics=accuracy,\n",
    "    lr=0.1\n",
    ")\n",
    "\n",
    "# then we call the `fit` method passing the number of epochs (10) as a parameter\n",
    "learner.fit(20)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "interpreter": {
   "hash": "1114d05621ef29e6570c6f64184b372ef2b76e47e3b31dc67323ed4052f14a39"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}